{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Incremental loading with dlt\n",
    "The goal: download only trips made after June 15, 2009, skipping the old ones.\n",
    "\n",
    "Using dlt, we set up an incremental filter to only fetch trips made after a certain date:\n",
    "\n",
    "cursor_date = dlt.sources.incremental(\"Trip_Dropoff_DateTime\", initial_value=\"2009-06-15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells dlt:\n",
    "\n",
    "Start date: June 15, 2009 (initial_value).\n",
    "Field to track: Trip_Dropoff_DateTime (our timestamp).\n",
    "As you run the pipeline repeatedly, dlt will keep track of the latest Trip_Dropoff_DateTime value processed. It will skip records older than this date in future runs.\n",
    "\n",
    "Let's make the data resource incremental using dlt.sources.incremental:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "\n",
    "\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"append\")\n",
    "def ny_taxi(\n",
    "    cursor_date=dlt.sources.incremental(\n",
    "        \"Trip_Dropoff_DateTime\",   # <--- field to track, our timestamp\n",
    "        initial_value=\"2009-06-15\",   # <--- start date June 15, 2009\n",
    "        )\n",
    "    ):\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page=1,\n",
    "            total_path=None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for page in client.paginate(\"data_engineering_zoomcamp_api\"):\n",
    "        yield page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run our pipeline and load the fresh taxi rides data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at 2025-02-18 19:05:20.296412+00:00 and COMPLETED in 33.24 seconds with 4 steps.\n",
      "Step extract COMPLETED in 30.77 seconds.\n",
      "\n",
      "Load package 1739905521.5706 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 0.06 seconds.\n",
      "No data found to normalize\n",
      "\n",
      "Step load COMPLETED in 1.17 seconds.\n",
      "Pipeline ny_taxi load step completed in ---\n",
      "0 load package(s) were loaded to destination duckdb and into dataset None\n",
      "The duckdb destination used duckdb:////workspaces/DEZoomCamp2025/workshop/ny_taxi.duckdb location to store data\n",
      "\n",
      "Step run COMPLETED in 33.24 seconds.\n",
      "Pipeline ny_taxi load step completed in ---\n",
      "0 load package(s) were loaded to destination duckdb and into dataset None\n",
      "The duckdb destination used duckdb:////workspaces/DEZoomCamp2025/workshop/ny_taxi.duckdb location to store data\n"
     ]
    }
   ],
   "source": [
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(pipeline_name=\"ny_taxi\", destination=\"duckdb\", dataset_name=\"ny_taxi_data\")\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(ny_taxi)\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 5325 rows were flitered out and loaded into the duckdb destination. Let's take a look at the earliest date in the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2009, 6, 15, 0, 6, tzinfo=<UTC>),)]\n"
     ]
    }
   ],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    res = client.execute_sql(\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "            MIN(trip_dropoff_date_time)\n",
    "            FROM rides;\n",
    "            \"\"\"\n",
    "        )\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at 2025-02-18 19:06:14.081040+00:00 and COMPLETED in 29.58 seconds with 4 steps.\n",
      "Step extract COMPLETED in 28.73 seconds.\n",
      "\n",
      "Load package 1739905574.9094021 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 0.05 seconds.\n",
      "No data found to normalize\n",
      "\n",
      "Step load COMPLETED in 0.02 seconds.\n",
      "Pipeline ny_taxi load step completed in ---\n",
      "0 load package(s) were loaded to destination duckdb and into dataset None\n",
      "The duckdb destination used duckdb:////workspaces/DEZoomCamp2025/workshop/ny_taxi.duckdb location to store data\n",
      "\n",
      "Step run COMPLETED in 29.58 seconds.\n",
      "Pipeline ny_taxi load step completed in ---\n",
      "0 load package(s) were loaded to destination duckdb and into dataset None\n",
      "The duckdb destination used duckdb:////workspaces/DEZoomCamp2025/workshop/ny_taxi.duckdb location to store data\n"
     ]
    }
   ],
   "source": [
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(pipeline_name=\"ny_taxi\", destination=\"duckdb\", dataset_name=\"ny_taxi_data\")\n",
    "\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(ny_taxi)\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will detect that there are no new records based on the Trip_Dropoff_DateTime field and the incremental cursor. As a result, no new data will be loaded into the destination:\n",
    "\n",
    "0 load package(s) were loaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
