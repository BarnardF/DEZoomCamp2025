{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Loading data into a Data Warehouse (BigQuery)\n",
    "First, install the dependencies, define the source, then change the destination name and run the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install dlt[bigquery]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our NY Taxi API and load data from the source into destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "\n",
    "\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
    "def ny_taxi():\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://us-central1-dlthub-analytics.cloudfunctions.net\",\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page=1,\n",
    "            total_path=None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for page in client.paginate(\"data_engineering_zoomcamp_api\"):\n",
    "        yield page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a destination\n",
    "\n",
    "Switching between data warehouses (BigQuery, Snowflake, Redshift) or data lakes (S3, Google Cloud Storage, Parquet files) in dlt is incredibly straightforward â€” simply modify the destination parameter in your pipeline configuration.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='taxi_data',\n",
    "    destination='duckdb', # <--- to test pipeline locally\n",
    "    dataset_name='taxi_rides',\n",
    ")\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='taxi_data',\n",
    "    destination='bigquery', # <--- to run pipeline in production\n",
    "    dataset_name='taxi_rides',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flexibility allows you to easily transition from local development to production-grade environments.\n",
    "\n",
    "ðŸ’¡ No need to rewrite your pipeline â€” dlt adapts automatically!\n",
    "\n",
    "Set Credentials\n",
    "\n",
    "The next logical step is to set credentials using dlt's TOML providers or environment variables (ENVs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import toml\n",
    "\n",
    "# Load the secrets.toml file\n",
    "config = toml.load('.dlt/secrets.toml')\n",
    "\n",
    "# Extract BigQuery credentials\n",
    "bigquery_credentials = config['destination']['bigquery']['credentials']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"taxi_data\",\n",
    "    destination=\"bigquery\",\n",
    "    dataset_name=\"taxi_rides\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "info = pipeline.run(ny_taxi)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Whatâ€™s different?\n",
    "\n",
    "dlt automatically adapts the schema to fit BigQuery.\n",
    "Partitioning & clustering can be applied for performance optimization.\n",
    "Efficient batch loading ensures scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
